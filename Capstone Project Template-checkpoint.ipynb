{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Statsbomb open data engineering project \n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Stasbomb have made certain leagues of their Data freely available for public use for research projects .\n",
    "I have tried to get use of such data by scraping the files content from the github repository.\n",
    "https://github.com/statsbomb/open-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![sb_color.jpg](/statsbomb-project/sb_color-checkpoint.jpg/sb_color.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### STATSBOMB LOGO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "##### Extraction\n",
    "I will scrape the data content from the github repository for events and matches datasets.\n",
    "Then the data will be uploaded to S3 buckets as staging area.\n",
    "\n",
    "##### Transformation \n",
    "Then the data will be wrangled and cleaned using spark and I will use different spark functionalities to model the data.\n",
    "\n",
    "##### Loading \n",
    "The modeled data will be finally uploaded to Redshift.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "I will scrape available on github (https://github.com/statsbomb/open-data)\n",
    "I am targeting matches and events files .\n",
    "Matches datasets contain data realted to the match as score, team names , statdium , etc..\n",
    "Events datasets contain data realted to all events in a match and player and team affected\n",
    "by or applying such action.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "![pipeline](pipeline.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "\n",
    "competitons_data_url= ('https://raw.githubusercontent.com/statsbomb/open-data/master/data/competitions.json')\n",
    "events_data_url     = ('https://raw.githubusercontent.com/statsbomb/open-data/master/data/events/{}')\n",
    "lineups_data_url    = ('https://raw.githubusercontent.com/statsbomb/open-data/master/data/lineups/{}')\n",
    "matches_folders_url = ('https://github.com/statsbomb/open-data/tree/master/data/matches/{}')\n",
    "matches_data_url    = ('https://raw.githubusercontent.com/statsbomb/open-data/master/data/matches/{}/{}')\n",
    "\n",
    "\n",
    "# URL on the statsbomb Github repo where the matches json files are stored\n",
    "events_github_url = 'https://github.com/statsbomb/open-data/tree/master/data/events'  \n",
    "matches_github_url = 'https://github.com/statsbomb/open-data/tree/master/data/matches'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a list of the free availble events_matches_list id\n",
    "\n",
    "result = requests.get(events_github_url)\n",
    "\n",
    "soup = BeautifulSoup(result.text, 'html.parser')\n",
    "jfiles = soup.find_all(title=re.compile(\"\\.json$\"))\n",
    "\n",
    "events_matches_list  = [ ]\n",
    "for i in jfiles:\n",
    "        events_matches_list.append(i.extract().get_text())\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#matches folders is number / json file, so we need 2 lists : folder number list  and json files list \n",
    "result = requests.get(matches_github_url)\n",
    "\n",
    "soup = BeautifulSoup(result.text, 'html.parser')\n",
    "jfiles = soup.find_all(title=re.compile(\"^[0-9]+$\"))\n",
    "\n",
    "id_matches_list  = [ ]\n",
    "for i in jfiles:\n",
    "        id_matches_list.append(i.extract().get_text())\n",
    "\n",
    "# get successfull response status only as the id of the folders number lists\n",
    "id_matches_folders =[]\n",
    "for i in id_matches_list :\n",
    "    response_API = requests.get(matches_folders_url.format(i))\n",
    "    if (response_API.status_code == 200):\n",
    "        id_matches_folders.append (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary of all json files with folder number as their key \n",
    "folder_json_matches_dict  ={}\n",
    "\n",
    "for _id in id_matches_folders:\n",
    "    result = requests.get(matches_folders_url.format(_id))\n",
    "    \n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    jfiles = soup.find_all(title=re.compile(\"\\.json$\"))\n",
    "    val=[]\n",
    "    for i in jfiles:\n",
    "        val.append(i.extract().get_text())\n",
    "        \n",
    "    folder_json_matches_dict[_id]= val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Create a function to upload files to s3 buckets (boto3) withouting downloading the files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# intializing a boto3 resource to S3 aws service and setting my access and secret keys\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=aws_access_key_id,\n",
    "                       aws_secret_access_key=aws_access_key_id )\n",
    "\n",
    "bucket_name = 'statsbomb-project'\n",
    "folder ='data/'\n",
    "\n",
    "def upload_to_s3 (url_response ,file_name ):\n",
    "    s3object = s3.Object(bucket_name,  ''.join([folder,file_name]))\n",
    "    s3object.put(    Body=(bytes(json.dumps(url_response.json()).encode('UTF-8'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "#### Events upload containing teams and players\n",
    "#upload events data files (taking a sample of the list ) in folder ='data/events/' \n",
    "for _id in events_matches_list[:4]:\n",
    "    events_data_response      = requests.get    (events_data_url.format(_id))\n",
    "    upload_to_s3( events_data_response , ''.join(['events/',_id]) )\n",
    "    #\n",
    "    #events_data_response.status_code \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "#upload matches data files (taking a sample of the list ) in folder ='data/matches/foldernumber' \n",
    "for _id in folder_json_matches_dict:\n",
    "    if (_id =='43'or _id=='37'):## added to get sample folder only with 2 json files (this line would be removed to upload the whole dateset)\n",
    "        for js in folder_json_matches_dict[_id]:\n",
    "            matches_data_response      = requests.get    (matches_data_url.format(_id,js))\n",
    "            upload_to_s3( matches_data_response , ''.join(['matches/{}/'.format(_id),js]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### below code was workaround solution for some libraries comaptibility issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID='xxxxxxx'\n",
    "AWS_SECRET_ACCESS_KEY='xxxxxxx'\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] =  AWS_ACCESS_KEY_ID\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = AWS_SECRET_ACCESS_KEY\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\",AWS_ACCESS_KEY_ID)\\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\",AWS_SECRET_ACCESS_KEY)\\\n",
    "        .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_ev = spark.read.json(\"s3a://statsbomb-project/data/events/15956.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_ev.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_ev.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad_behaviour</th>\n",
       "      <th>ball_receipt</th>\n",
       "      <th>ball_recovery</th>\n",
       "      <th>block</th>\n",
       "      <th>carry</th>\n",
       "      <th>clearance</th>\n",
       "      <th>counterpress</th>\n",
       "      <th>dribble</th>\n",
       "      <th>duel</th>\n",
       "      <th>duration</th>\n",
       "      <th>...</th>\n",
       "      <th>possession_team</th>\n",
       "      <th>related_events</th>\n",
       "      <th>second</th>\n",
       "      <th>shot</th>\n",
       "      <th>substitution</th>\n",
       "      <th>tactics</th>\n",
       "      <th>team</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>type</th>\n",
       "      <th>under_pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>([79.8, 58.1],)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.653373</td>\n",
       "      <td>...</td>\n",
       "      <td>(217, Barcelona)</td>\n",
       "      <td>[29628eb2-d838-4903-b2a5-dfe2fd4018fd, 79f8187...</td>\n",
       "      <td>18</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>(217, Barcelona)</td>\n",
       "      <td>00:22:18.699</td>\n",
       "      <td>(43, Carry)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.858506</td>\n",
       "      <td>...</td>\n",
       "      <td>(217, Barcelona)</td>\n",
       "      <td>[b526623f-a155-4bb1-a914-b5ceb39ff5f6]</td>\n",
       "      <td>21</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>(217, Barcelona)</td>\n",
       "      <td>00:22:21.353</td>\n",
       "      <td>(30, Pass)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>(217, Barcelona)</td>\n",
       "      <td>[29628eb2-d838-4903-b2a5-dfe2fd4018fd]</td>\n",
       "      <td>22</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>(217, Barcelona)</td>\n",
       "      <td>00:22:22.211</td>\n",
       "      <td>(42, Ball Receipt*)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  bad_behaviour ball_receipt ball_recovery block            carry clearance  \\\n",
       "0          None         None          None  None  ([79.8, 58.1],)      None   \n",
       "1          None         None          None  None             None      None   \n",
       "2          None         None          None  None             None      None   \n",
       "\n",
       "  counterpress dribble  duel  duration      ...         possession_team  \\\n",
       "0         None    None  None  2.653373      ...        (217, Barcelona)   \n",
       "1         None    None  None  0.858506      ...        (217, Barcelona)   \n",
       "2         None    None  None       NaN      ...        (217, Barcelona)   \n",
       "\n",
       "                                      related_events second  shot  \\\n",
       "0  [29628eb2-d838-4903-b2a5-dfe2fd4018fd, 79f8187...     18  None   \n",
       "1             [b526623f-a155-4bb1-a914-b5ceb39ff5f6]     21  None   \n",
       "2             [29628eb2-d838-4903-b2a5-dfe2fd4018fd]     22  None   \n",
       "\n",
       "   substitution tactics              team     timestamp                 type  \\\n",
       "0          None    None  (217, Barcelona)  00:22:18.699          (43, Carry)   \n",
       "1          None    None  (217, Barcelona)  00:22:21.353           (30, Pass)   \n",
       "2          None    None  (217, Barcelona)  00:22:22.211  (42, Ball Receipt*)   \n",
       "\n",
       "  under_pressure  \n",
       "0           None  \n",
       "1           None  \n",
       "2           None  \n",
       "\n",
       "[3 rows x 37 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ev.filter(df_ev['index'] > 1000).limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|player                        |\n",
      "+------------------------------+\n",
      "|[11293, Fernando Calero Villa]|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ev.select('player').filter (df_ev.index=='55').show(truncate =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# |-- player: struct (nullable = true)\n",
    "# |    |-- id: long (nullable = true)\n",
    "# |    |-- name: string (nullable = tru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "players = df_ev.selectExpr(\"team['id'] as  team_id\"  ,\"player['id'] as  player_id\" , \"player['name'] as  player_name\" \n",
    "                           ,\"position['id'] as position_id\", \"position['name'] as position_name\" ) \n",
    "\n",
    "players.columns\n",
    "\n",
    "players.count()\n",
    "\n",
    "players= players.dropna()\n",
    "players.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "As we can see the schema is ugly and data values itself contains nested data .\n",
    "for example player column include two keys id , name each containing its own value.\n",
    "In addition we can seel some nulls.\n",
    "#### Cleaning Steps\n",
    "I have used spark to created new columns from the nested values and dropped duplicated values \n",
    "all that are saved as a view ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "we will have 3 main dimensions teams ,players and matches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing  data modelling  and  cleaning tasks here\n",
    "\n",
    "\n",
    "def get_teams_from_events(file):\n",
    "    df_ev = spark.read.json(\"s3a://statsbomb-project/data/events/{}\".format(file))\n",
    "    teams = df_ev.selectExpr(\"team['id'] as  team_id\"   , \"team['name'] as  team_name\" )\n",
    "    teams .createOrReplaceTempView('teams')\n",
    "    teams = spark.sql    (\"\"\"\\\n",
    "    select distinct team_id , team_name from  teams \\\n",
    "    \"\"\")\n",
    "    return teams\n",
    "    \n",
    "\n",
    "def get_players_from_events(file):\n",
    "    df_ev = spark.read.json(\"s3a://statsbomb-project/data/events/{}\".format(file))\n",
    "    players = df_ev.selectExpr(\"team['id'] as  team_id\"  ,\"player['id'] as  player_id\" , \"player['name'] as  player_name\" \n",
    "                           ,\"position['id'] as position_id\", \"position['name'] as position_name\" ) \n",
    "    players .createOrReplaceTempView('players')\n",
    "\n",
    "    players = spark.sql    (\"\"\"\\\n",
    "    select distinct team_id , player_id,player_name ,position_id , position_name from  players \\\n",
    "    \"\"\")\n",
    "    return players\n",
    "    \n",
    "def clean_df (df):\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def upload_cleaned_df(df,folder):\n",
    "    df.write.format('csv').option('header','true').save('s3a://statsbomb-project/data/{}'.format(folder),mode='append')\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#loop over our json files list to wranlge one by one and then upload\n",
    "# but we will use sample of our whole list \n",
    "\n",
    "for ev in events_matches_list[2:4]:\n",
    "\n",
    "    df_teams=get_teams_from_events(ev)\n",
    "    df_players=get_players_from_events(ev)\n",
    "\n",
    "    df_teams = clean_df(df_teams)\n",
    "    df_players = clean_df(df_players)\n",
    "\n",
    "    upload_cleaned_df(df_teams,'teams_table')\n",
    "    upload_cleaned_df(df_players,'players_table')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_matches(folder , file):\n",
    "    df_mt = spark.read.json(\"s3a://statsbomb-project/data/matches/{}/{}\".format(folder , file))\n",
    "    matches = df_mt.selectExpr\\\n",
    "    (\"competition['competition_id'] as competition_id\",'match_id',\\\n",
    "     \"home_team['home_team_id'] as home_team_id\" ,\"away_team['away_team_id']  as away_team_id\",\\\n",
    "     'away_score', 'home_score', \"stadium['id'] as stadium_id \", \"stadium['name'] as stadium_name\",\\\n",
    "     \"stadium['country']['name'] as stadium_country_name\")\n",
    "    \n",
    "    matches .createOrReplaceTempView('matches')\n",
    "\n",
    "    matches = spark.sql    (\"\"\"\\\n",
    "    select distinct  \\\n",
    "         match_id ,  \\\n",
    "         home_team_id ,\\\n",
    "         away_team_id ,\\\n",
    "         away_score ,\\\n",
    "         home_score ,\\\n",
    "         stadium_id ,\\\n",
    "         stadium_name ,\\\n",
    "         stadium_country_name  from  matches \\\n",
    "    \"\"\")\n",
    "    return matches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "for folder in folder_json_matches_dict:\n",
    "    if (folder =='43'or folder=='37'):## added to get sample folder only with 2 json files (this line would be removed to upload the whole dateset)\n",
    "        for js in folder_json_matches_dict[folder]:\n",
    "            df_mt = get_matches(folder , js)\n",
    "            df_mt = clean_df(df_mt)\n",
    "            upload_cleaned_df (df_mt, 'matches_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "There 3 main files used for the pipelines as follows:\n",
    "* sql_queries_statsbomb.py  \n",
    "* create_tbls_statsbomb.py   \n",
    "* etl_statsbomb.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here\n",
    "# Run the create_tbls_statsbomb.py from  console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "# Run quality check file in redshift query editor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "As the data is get from statsbomb repo the data dictionary can be found in below repos:\n",
    "\n",
    "* for events dataset : https://github.com/statsbomb/open-data/blob/master/doc/Open%20Data%20Events%20v4.0.0.pdf\n",
    "* for matches dataset : https://github.com/statsbomb/open-data/blob/master/doc/Open%20Data%20Matches%20v3.0.0.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "If The data was increased \n",
    "\n",
    "\n",
    "* Spark standalone mode will be optimize manipulating our data very much as the data wil be distributed across a clsuter of nodes instead of one local machine.\n",
    "We rent a cluster of machines, i.e., our Spark Cluster, and iti s located in AWS data centers. We rent these using AWS service called Elastic Compute Cloud (EC2).\n",
    "We log in from your local computer to this Spark cluster.\n",
    "Upon running our Spark code, the cluster will load the dataset from Amazon S3 into the cluster’s memory distributed across each machine in the cluster.\n",
    "\n",
    "* In our case as the data source is a github repo we can directly clone the repo in the EC2 cluster and manipulate the data.\n",
    "\n",
    "And airflow would become handy to maipulate ou sql queries so that we can  run pipeline with execution context capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
